# æ”¶ç›Šæ± æ–‡æ¡£ç³»ç»Ÿ

Web3 DeFi æ”¶ç›Šæ± æŠ•èµ„é¡¹ç›®çš„ç»“æ„åŒ–æ–‡æ¡£ç³»ç»Ÿï¼Œæ”¯æŒ Obsidian å¯è§†åŒ–å±•ç¤ºå’Œç¨‹åºåŒ–è§£æã€‚

## ğŸ“ ç›®å½•ç»“æ„

```
pools/
â”œâ”€â”€ cex/              # ä¸­å¿ƒåŒ–äº¤æ˜“æ‰€äº§å“
â”œâ”€â”€ chain/            # é“¾ä¸Š DeFi åè®®
â””â”€â”€ README.md         # æœ¬æ–‡æ¡£ï¼ˆåŒ…å«å®Œæ•´è§£æä»£ç ï¼‰
```

## ğŸ“‹ æ–‡æ¡£æ ¼å¼è§„èŒƒ

### æ ‡å‡†æ ¼å¼ç¤ºä¾‹

```markdown
---
Protocol: "[[Aave]]"
Type: Deposit
Chain:
  - BSC
  - Arbitrum
Risk: æ— é£é™©
Token: USDT
APR-Low: 0.02
APR-High: 0.08
Market:
  - é€šç”¨
  - ç‰›å¸‚
---

## ğŸ“Š æ± å­è¯¦æƒ…

**Underlying**

å­˜æ¬¾é‡‘åº“ç±»äº§å“ï¼Œèµ„é‡‘è¿›å…¥å®˜æ–¹å‚¨å¤‡æ± ï¼Œé€šè¿‡å€Ÿè´·åˆ©å·®è·å¾—æ”¶ç›Šã€‚æ”¯æŒéšæ—¶å­˜å–ï¼Œä½†æ”¶ç›Šç‡ä¼šæ ¹æ®å¸‚åœºèµ„é‡‘ä¾›éœ€åŠ¨æ€è°ƒæ•´ã€‚

**Danger**

ä¸»è¦é£é™©æ¥è‡ªæ™ºèƒ½åˆçº¦æ¼æ´å’Œæ²»ç†é£é™©ã€‚å†å²ä¸Šæœªå‘ç”Ÿé‡å¤§å®‰å…¨äº‹ä»¶ï¼Œä½†å­˜åœ¨ç†è®ºä¸Šçš„ä»£ç é£é™©ã€‚

**Scenarios**

- **ä¿æœ¬ U æœ¬ä½ç†è´¢**: é€‚åˆå¸Œæœ›è·å¾—ç¨³å®šæ”¶ç›Šçš„ä¿å®ˆæŠ•èµ„è€…
- **æŒå¸ç”Ÿæ¯**: é•¿æœŸæŒå¸ç”¨æˆ·çš„ç†æƒ³é€‰æ‹©ï¼Œæ— éœ€é¢‘ç¹æ“ä½œ
- **ç†Šå¸‚é¿é™©**: åœ¨å¸‚åœºä¸ç¡®å®šæ€§è¾ƒé«˜æ—¶çš„èµ„é‡‘é¿é£æ¸¯

**Remark**  
é“¾ä¸Šå€Ÿè´·å·¨å¤´ï¼Œé€‚åˆç†Šå¸‚ç¯å¢ƒä¸‹çš„ç¨³å¥æŠ•èµ„ã€‚å»ºè®®åˆ†æ‰¹å»ºä»“ï¼Œé¿å…ä¸€æ¬¡æ€§å¤§é¢æŠ•å…¥ã€‚

---

## å…¶ä»–è¯´æ˜

è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šè¯¦ç»†è¯´æ˜...
```

## ğŸ“Š å­—æ®µè¯´æ˜

### Frontmatter å­—æ®µï¼ˆObsidian ç´¢å¼•ï¼‰

| å­—æ®µ | ç±»å‹ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|------|
| Protocol | String | åè®®åç§° | `"[[Aave]]"` |
| Type | String | äº§å“ç±»å‹ | `Deposit`, `LP`, `Staking` |
| Chain | Array | æ”¯æŒçš„åŒºå—é“¾ | `["BSC", "Ethereum"]` |
| Risk | String | é£é™©ç­‰çº§ | `æ— é£é™©`, `ä½é£é™©`, `é«˜é£é™©` |
| Token | String | ä¸»è¦ä»£å¸ | `USDT`, `ETH`, `BTC` |
| APR-Low | Number | æœ€ä½å¹´åŒ–æ”¶ç›Šç‡ | `0.02` (2%) |
| APR-High | Number | æœ€é«˜å¹´åŒ–æ”¶ç›Šç‡ | `0.15` (15%) |
| Market | Array | é€‚ç”¨å¸‚åœºç¯å¢ƒ | `["é€šç”¨", "ç‰›å¸‚", "ç†Šå¸‚"]` |

### æ­£æ–‡è¯¦æƒ…å­—æ®µï¼ˆè‡ªç”± Markdown æ–‡æœ¬ï¼‰

| å­—æ®µ | ç±»å‹ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|------|
| Underlying | Markdown | æ”¶ç›Šæœºåˆ¶è¯¦ç»†è¯´æ˜ | æ®µè½æ–‡æœ¬ã€åˆ—è¡¨ã€é“¾æ¥ç­‰è‡ªç”±æ ¼å¼ |
| Danger | Markdown | é£é™©åˆ†æå’Œæ³¨æ„äº‹é¡¹ | æ®µè½æ–‡æœ¬ã€åˆ—è¡¨ã€é“¾æ¥ç­‰è‡ªç”±æ ¼å¼ |
| Scenarios | Markdown | ä½¿ç”¨åœºæ™¯å’Œç­–ç•¥å»ºè®® | æ®µè½æ–‡æœ¬ã€åˆ—è¡¨ã€é“¾æ¥ç­‰è‡ªç”±æ ¼å¼ |
| Remark | Markdown | å¤‡æ³¨å’Œè¡¥å……è¯´æ˜ | æ®µè½æ–‡æœ¬ã€åˆ—è¡¨ã€é“¾æ¥ç­‰è‡ªç”±æ ¼å¼ |

## ğŸ Python è§£æå™¨

å°†ä»¥ä¸‹ä»£ç ä¿å­˜ä¸º `pool_parser.py`ï¼š

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¶ç›Šæ± æ–‡æ¡£è§£æå™¨ - Python ç‰ˆæœ¬
ä¾èµ–: pip install pyyaml markdown
"""

import re
import yaml
import json
import markdown
from typing import Dict, List, Any, Tuple, Optional
from pathlib import Path
from dataclasses import dataclass, asdict
from datetime import datetime

@dataclass
class PoolFrontmatter:
    """æ± å­å‰ç½®å±æ€§æ•°æ®ç±»"""
    Protocol: str = ""
    Type: str = ""
    Chain: List[str] = None
    Risk: str = ""
    Token: str = ""
    APR_Low: float = 0.0
    APR_High: float = 0.0
    Market: List[str] = None
    
    def __post_init__(self):
        if self.Chain is None:
            self.Chain = []
        if self.Market is None:
            self.Market = []

@dataclass 
class PoolDetails:
    """æ± å­è¯¦æƒ…æ•°æ®ç±»"""
    Underlying: str = ""
    Danger: str = ""
    Scenarios: str = ""
    Remark: str = ""

@dataclass
class PoolDocument:
    """å®Œæ•´æ± å­æ–‡æ¡£æ•°æ®ç±»"""
    file_path: str = ""
    file_name: str = ""
    category: str = ""
    frontmatter: PoolFrontmatter = None
    details: PoolDetails = None
    parse_time: str = ""
    
    def __post_init__(self):
        if self.frontmatter is None:
            self.frontmatter = PoolFrontmatter()
        if self.details is None:
            self.details = PoolDetails()
        if not self.parse_time:
            self.parse_time = datetime.now().isoformat()

class PoolDocumentParser:
    """æ”¶ç›Šæ± æ–‡æ¡£è§£æå™¨"""
    
    def __init__(self):
        self.detail_fields = ['Underlying', 'Danger', 'Scenarios', 'Remark']
        self.parse_stats = {'total_parsed': 0, 'successful': 0, 'failed': 0, 'errors': []}
    
    def parse_frontmatter(self, frontmatter_text: str) -> Dict[str, Any]:
        """è§£æ YAML frontmatter"""
        try:
            data = yaml.safe_load(frontmatter_text) or {}
            # å¤„ç†ç‰¹æ®Šå­—æ®µåï¼ˆå¸¦è¿å­—ç¬¦çš„ï¼‰
            if 'APR-Low' in data:
                data['APR_Low'] = float(data.pop('APR-Low', 0))
            if 'APR-High' in data:
                data['APR_High'] = float(data.pop('APR-High', 0))
            # Market å­—æ®µå·²ç§»è‡³ frontmatterï¼Œæ— éœ€ç‰¹æ®Šå¤„ç†
            return data
        except (yaml.YAMLError, ValueError) as e:
            self.parse_stats['errors'].append(f"YAML è§£æé”™è¯¯: {e}")
            return {}
    
    def parse_details(self, body: str) -> Dict[str, Any]:
        """è§£ææ­£æ–‡è¯¦æƒ… - ä½¿ç”¨æ ‡å‡† Markdown è§£æåº“"""
        details = {}
        
        try:
            # ä½¿ç”¨æ ‡å‡† Markdown è§£æåº“è§£æå†…å®¹
            md = markdown.Markdown()
            html_content = md.convert(body)
            
            # åŸºäº HTML è§£ææå–å­—æ®µå†…å®¹
            for field in self.detail_fields:
                content = self._extract_field_from_html(html_content, field)
                if content:
                    # å°† HTML è½¬å› Markdown æ ¼å¼ç”¨äºå­˜å‚¨
                    details[field] = content
                else:
                    details[field] = ''
        except Exception as e:
            # é™çº§åˆ°æ­£åˆ™è¡¨è¾¾å¼æ–¹æ³•ä½œä¸ºå¤‡ä»½
            self.parse_stats['errors'].append(f"Markdown è§£æå¤±è´¥ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼: {e}")
            for field in self.detail_fields:
                pattern = rf'\*\*{field}\*\*\s*\n\n(.+?)(?=\n\*\*|\n---|\\n##|$)'
                match = re.search(pattern, body, re.MULTILINE | re.DOTALL)
                if match:
                    details[field] = match.group(1).strip()
                else:
                    # å°è¯•æ²¡æœ‰ç©ºè¡Œçš„æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
                    pattern = rf'\*\*{field}\*\*\s*\n(.+?)(?=\n\*\*|\n---|\\n##|$)'
                    match = re.search(pattern, body, re.MULTILINE | re.DOTALL)
                    details[field] = match.group(1).strip() if match else ''
        
        return details
    
    def _extract_field_from_html(self, html_content: str, field_name: str) -> str:
        """ä» HTML å†…å®¹ä¸­æå–æŒ‡å®šå­—æ®µçš„å†…å®¹"""
        import re
        from html import unescape
        
        # æŸ¥æ‰¾å­—æ®µæ ‡é¢˜å’Œå†…å®¹
        pattern = rf'<strong>{field_name}</strong>\s*</p>\s*(.*?)(?=<p>\s*<strong>(?:Underlying|Danger|Scenarios|Remark)</strong>|<hr|$)'
        match = re.search(pattern, html_content, re.DOTALL | re.IGNORECASE)
        
        if match:
            content = match.group(1).strip()
            # æ¸…ç†HTMLå¹¶è¿”å›çº¯æ–‡æœ¬æˆ–ä¿ç•™åŸºæœ¬æ ¼å¼
            return unescape(content)
        
        return ''
    
    def parse_file(self, file_path: str) -> Optional[PoolDocument]:
        """è§£æå•ä¸ªæ± å­æ–‡æ¡£"""
        self.parse_stats['total_parsed'] += 1
        try:
            path_obj = Path(file_path)
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åˆ†ç¦» frontmatter å’Œ body
            frontmatter_match = re.match(r'^---\n(.*?)\n---\n(.*)$', content, re.DOTALL)
            if not frontmatter_match:
                self.parse_stats['errors'].append(f"æ— æ•ˆæ ¼å¼: {file_path}")
                self.parse_stats['failed'] += 1
                return None
            
            frontmatter_text, body = frontmatter_match.groups()
            
            # è§£ææ•°æ®
            frontmatter_data = self.parse_frontmatter(frontmatter_text)
            details_data = self.parse_details(body)
            
            # åˆ›å»ºæ•°æ®å¯¹è±¡
            frontmatter = PoolFrontmatter(**{k: v for k, v in frontmatter_data.items() 
                                           if k in PoolFrontmatter.__dataclass_fields__})
            details = PoolDetails(**details_data)
            
            document = PoolDocument(
                file_path=str(path_obj.resolve()),
                file_name=path_obj.name,
                category=path_obj.parent.name,
                frontmatter=frontmatter,
                details=details
            )
            
            self.parse_stats['successful'] += 1
            return document
            
        except Exception as e:
            error_msg = f"è§£æå¤±è´¥ {file_path}: {e}"
            self.parse_stats['errors'].append(error_msg)
            self.parse_stats['failed'] += 1
            return None
    
    def parse_directory(self, pools_dir: str, recursive: bool = True) -> List[PoolDocument]:
        """æ‰¹é‡è§£æç›®å½•ä¸‹çš„æ‰€æœ‰æ± å­æ–‡æ¡£"""
        results = []
        pools_path = Path(pools_dir)
        
        if not pools_path.exists():
            raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨: {pools_dir}")
        
        pattern = "**/*.md" if recursive else "*.md"
        for md_file in pools_path.glob(pattern):
            if md_file.name.lower() in ['readme.md']:
                continue
            document = self.parse_file(str(md_file))
            if document:
                results.append(document)
        
        return results
    
    def filter_by_category(self, documents: List[PoolDocument], category: str) -> List[PoolDocument]:
        """æŒ‰ç±»åˆ«ç­›é€‰æ–‡æ¡£"""
        return [doc for doc in documents if doc.category == category]
    
    def filter_by_risk(self, documents: List[PoolDocument], risk: str) -> List[PoolDocument]:
        """æŒ‰é£é™©ç­‰çº§ç­›é€‰æ–‡æ¡£"""
        return [doc for doc in documents if doc.frontmatter.Risk == risk]
    
    def get_statistics(self, documents: List[PoolDocument]) -> Dict[str, Any]:
        """è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯"""
        if not documents:
            return {}
        
        stats = {
            'total_count': len(documents),
            'categories': {},
            'risk_distribution': {},
            'token_distribution': {}
        }
        
        for doc in documents:
            # ç±»åˆ«åˆ†å¸ƒ
            category = doc.category
            stats['categories'][category] = stats['categories'].get(category, 0) + 1
            # é£é™©åˆ†å¸ƒ
            risk = doc.frontmatter.Risk
            stats['risk_distribution'][risk] = stats['risk_distribution'].get(risk, 0) + 1
            # ä»£å¸åˆ†å¸ƒ
            token = doc.frontmatter.Token
            stats['token_distribution'][token] = stats['token_distribution'].get(token, 0) + 1
        
        return stats
    
    def export_to_json(self, documents: List[PoolDocument]) -> str:
        """å¯¼å‡ºä¸º JSON æ ¼å¼"""
        serializable_docs = [asdict(doc) for doc in documents]
        return json.dumps(serializable_docs, ensure_ascii=False, indent=2)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    parser = PoolDocumentParser()
    
    # è§£æå•ä¸ªæ–‡ä»¶
    doc = parser.parse_file("cex/Binance BNSOL Staking.md")
    if doc:
        print(f"åè®®: {doc.frontmatter.Protocol}")
        print(f"å¸‚åœº: {doc.frontmatter.Market}")
        print(f"æ”¶ç›Šæœºåˆ¶: {doc.details.Underlying[:50]}...")
    
    # æ‰¹é‡è§£æ
    all_docs = parser.parse_directory(".")
    print(f"å…±è§£æ {len(all_docs)} ä¸ªæ± å­æ–‡æ¡£")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = parser.get_statistics(all_docs)
    print("é£é™©åˆ†å¸ƒ:", stats.get('risk_distribution', {}))
```

## ğŸŸ¨ JavaScript è§£æå™¨

å°†ä»¥ä¸‹ä»£ç ä¿å­˜ä¸º `pool_parser.js`ï¼š

```javascript
#!/usr/bin/env node
/**
 * æ”¶ç›Šæ± æ–‡æ¡£è§£æå™¨ - JavaScript ç‰ˆæœ¬
 * ä¾èµ–: npm install remark remark-html gray-matter
 */

const fs = require('fs');
const path = require('path');
const { remark } = require('remark');
const remarkHtml = require('remark-html');

class PoolDocumentParser {
    constructor() {
        this.detailFields = ['Underlying', 'Danger', 'Scenarios', 'Remark'];
        this.parseStats = { totalParsed: 0, successful: 0, failed: 0, errors: [] };
    }
    
    /**
     * è§£æ YAML frontmatter
     */
    parseFrontmatter(frontmatterText) {
        try {
            const data = {};
            const lines = frontmatterText.split('\n');
            
            for (const line of lines) {
                const trimmed = line.trim();
                if (!trimmed || trimmed.startsWith('#')) continue;
                
                if (trimmed.includes(':')) {
                    const colonIndex = trimmed.indexOf(':');
                    const key = trimmed.substring(0, colonIndex).trim();
                    let value = trimmed.substring(colonIndex + 1).trim();
                    
                    // ç§»é™¤å¼•å·
                    if ((value.startsWith('"') && value.endsWith('"')) || 
                        (value.startsWith("'") && value.endsWith("'"))) {
                        value = value.slice(1, -1);
                    }
                    
                    // å¤„ç†ç‰¹æ®Šå­—æ®µå
                    if (key === 'APR-Low') {
                        data.APR_Low = parseFloat(value) || 0;
                    } else if (key === 'APR-High') {
                        data.APR_High = parseFloat(value) || 0;
                    } else {
                        data[key] = value;
                    }
                }
            }
            
            return data;
        } catch (error) {
            this.parseStats.errors.push(`YAML è§£æé”™è¯¯: ${error.message}`);
            return {};
        }
    }
    
    /**
     * è§£ææ­£æ–‡è¯¦æƒ… - ä½¿ç”¨æ ‡å‡† Markdown è§£æåº“
     */
    async parseDetails(body) {
        const details = {};
        
        try {
            // ä½¿ç”¨ remark è§£æ Markdown ä¸º AST
            const tree = remark().parse(body);
            
            // éå† AST æŸ¥æ‰¾å­—æ®µ
            for (const field of this.detailFields) {
                const markdownContent = this.extractFieldContent(tree, field);
                // å°†Markdownå†…å®¹è½¬æ¢ä¸ºHTML
                if (markdownContent.trim()) {
                    const htmlContent = await remark()
                        .use(remarkHtml, { sanitize: false })
                        .process(markdownContent);
                    details[field] = String(htmlContent).trim();
                } else {
                    details[field] = '';
                }
            }
        } catch (error) {
            // é™çº§åˆ°æ­£åˆ™è¡¨è¾¾å¼æ–¹æ³•ä½œä¸ºå¤‡ä»½
            this.parseStats.errors.push(`Markdown è§£æå¤±è´¥ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼: ${error.message}`);
            for (const field of this.detailFields) {
                const pattern = new RegExp(`\\*\\*${field}\\*\\*\\s*\\n\\n(.+?)(?=\\n\\*\\*|\\n---|\\n##|$)`, 'ms');
                const match = body.match(pattern);
                if (match) {
                    details[field] = match[1].trim();
                } else {
                    // å°è¯•æ²¡æœ‰ç©ºè¡Œçš„æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
                    const fallbackPattern = new RegExp(`\\*\\*${field}\\*\\*\\s*\\n(.+?)(?=\\n\\*\\*|\\n---|\\n##|$)`, 'ms');
                    const fallbackMatch = body.match(fallbackPattern);
                    details[field] = fallbackMatch ? fallbackMatch[1].trim() : '';
                }
            }
        }
        
        return details;
    }
    
    /**
     * ä» AST ä¸­æå–æŒ‡å®šå­—æ®µçš„å†…å®¹ï¼Œè¿”å›åŸå§‹Markdownæ ¼å¼
     */
    extractFieldContent(tree, fieldName) {
        const nodes = tree.children;
        let fieldStartIndex = -1;
        
        // æŸ¥æ‰¾å­—æ®µæ ‡é¢˜èŠ‚ç‚¹
        for (let i = 0; i < nodes.length; i++) {
            const node = nodes[i];
            if (node.type === 'paragraph' && node.children) {
                // æ£€æŸ¥æ˜¯å¦åŒ…å« **å­—æ®µå**
                const hasFieldTitle = node.children.some(child => 
                    child.type === 'strong' && 
                    child.children &&
                    child.children.some(grandchild => 
                        grandchild.type === 'text' && grandchild.value === fieldName
                    )
                );
                
                if (hasFieldTitle) {
                    fieldStartIndex = i;
                    break;
                }
            }
        }
        
        if (fieldStartIndex === -1) {
            return '';
        }
        
        // æ”¶é›†ä»å­—æ®µæ ‡é¢˜ååˆ°ä¸‹ä¸€ä¸ªå­—æ®µæ ‡é¢˜ä¹‹é—´çš„æ‰€æœ‰å†…å®¹
        const contentNodes = [];
        for (let i = fieldStartIndex + 1; i < nodes.length; i++) {
            const node = nodes[i];
            
            // æ£€æŸ¥æ˜¯å¦é‡åˆ°ä¸‹ä¸€ä¸ªå­—æ®µæ ‡é¢˜
            if (node.type === 'paragraph' && node.children) {
                const isNextField = node.children.some(child => 
                    child.type === 'strong' && 
                    child.children &&
                    child.children.some(grandchild => 
                        grandchild.type === 'text' && 
                        ['Underlying', 'Danger', 'Scenarios', 'Remark'].includes(grandchild.value)
                    )
                );
                
                if (isNextField) {
                    break;
                }
            }
            
            // æ£€æŸ¥æ˜¯å¦é‡åˆ°åˆ†éš”ç¬¦
            if (node.type === 'thematicBreak') {
                break;
            }
            
            contentNodes.push(node);
        }
        
        // å°†èŠ‚ç‚¹è½¬æ¢å›Markdownæ ¼å¼
        return contentNodes.map(node => this.nodeToMarkdown(node)).join('\n').trim();
    }
    
    /**
     * å°† AST èŠ‚ç‚¹è½¬æ¢ä¸º Markdown æ ¼å¼
     */
    nodeToMarkdown(node) {
        if (!node) return '';
        
        switch (node.type) {
            case 'text':
                return node.value || '';
            
            case 'paragraph':
                return (node.children || []).map(child => this.nodeToMarkdown(child)).join('');
            
            case 'strong':
                return `**${(node.children || []).map(child => this.nodeToMarkdown(child)).join('')}**`;
            
            case 'emphasis':
                return `*${(node.children || []).map(child => this.nodeToMarkdown(child)).join('')}*`;
            
            case 'link':
                const text = (node.children || []).map(child => this.nodeToMarkdown(child)).join('');
                return `[${text}](${node.url || ''})`;
            
            case 'list':
                return (node.children || []).map((item, index) => {
                    const itemText = this.nodeToMarkdown(item);
                    return node.ordered ? `${index + 1}. ${itemText}` : `- ${itemText}`;
                }).join('\n');
            
            case 'listItem':
                return (node.children || []).map(child => this.nodeToMarkdown(child)).join('');
            
            case 'code':
                return `\`\`\`\n${node.value || ''}\n\`\`\``;
            
            case 'inlineCode':
                return `\`${node.value || ''}\``;
            
            case 'heading':
                const level = '#'.repeat(node.depth || 1);
                const headingText = (node.children || []).map(child => this.nodeToMarkdown(child)).join('');
                return `${level} ${headingText}`;
            
            case 'blockquote':
                const quoteText = (node.children || []).map(child => this.nodeToMarkdown(child)).join('\n');
                return quoteText.split('\n').map(line => `> ${line}`).join('\n');
            
            default:
                if (node.children) {
                    return (node.children || []).map(child => this.nodeToMarkdown(child)).join('');
                }
                return node.value || '';
        }
    }
    
    /**
     * åˆ›å»ºæ ‡å‡†çš„å‰ç½®å±æ€§å¯¹è±¡
     */
    createFrontmatter(data = {}) {
        return {
            Protocol: data.Protocol || '',
            Type: data.Type || '',
            Chain: data.Chain || [],
            Risk: data.Risk || '',
            Token: data.Token || '',
            APR_Low: data.APR_Low || 0,
            APR_High: data.APR_High || 0,
            Market: data.Market || []
        };
    }
    
    /**
     * åˆ›å»ºæ ‡å‡†çš„è¯¦æƒ…å¯¹è±¡
     */
    createDetails(data = {}) {
        return {
            Underlying: data.Underlying || '',
            Danger: data.Danger || '',
            Scenarios: data.Scenarios || '',
            Remark: data.Remark || ''
        };
    }
    
    /**
     * è§£æå•ä¸ªæ± å­æ–‡æ¡£
     */
    async parseFile(filePath) {
        this.parseStats.totalParsed++;
        
        try {
            const absolutePath = path.resolve(filePath);
            const content = fs.readFileSync(filePath, 'utf-8');
            
            // åˆ†ç¦» frontmatter å’Œ body
            const frontmatterMatch = content.match(/^---\n(.*?)\n---\n(.*)$/s);
            if (!frontmatterMatch) {
                const error = `æ— æ•ˆæ ¼å¼: ${filePath}`;
                this.parseStats.errors.push(error);
                this.parseStats.failed++;
                return null;
            }
            
            const [, frontmatterText, body] = frontmatterMatch;
            
            // è§£ææ•°æ®
            const frontmatterData = this.parseFrontmatter(frontmatterText);
            const detailsData = await this.parseDetails(body);
            
            // åˆ›å»ºæ–‡æ¡£å¯¹è±¡
            const document = {
                filePath: absolutePath,
                fileName: path.basename(filePath),
                category: path.basename(path.dirname(filePath)),
                frontmatter: this.createFrontmatter(frontmatterData),
                details: this.createDetails(detailsData),
                parseTime: new Date().toISOString()
            };
            
            this.parseStats.successful++;
            return document;
            
        } catch (error) {
            const errorMsg = `è§£æå¤±è´¥ ${filePath}: ${error.message}`;
            this.parseStats.errors.push(errorMsg);
            this.parseStats.failed++;
            return null;
        }
    }
    
    /**
     * è·å–ç›®å½•ä¸‹çš„æ‰€æœ‰ .md æ–‡ä»¶
     */
    getMarkdownFiles(dir, recursive = true) {
        const files = [];
        
        const traverse = (currentDir) => {
            const entries = fs.readdirSync(currentDir);
            
            for (const entry of entries) {
                const fullPath = path.join(currentDir, entry);
                const stat = fs.statSync(fullPath);
                
                if (stat.isDirectory() && recursive) {
                    traverse(fullPath);
                } else if (stat.isFile() && entry.endsWith('.md') && 
                          !entry.toLowerCase().includes('readme')) {
                    files.push(fullPath);
                }
            }
        };
        
        if (fs.existsSync(dir)) {
            traverse(dir);
        }
        
        return files;
    }
    
    /**
     * æ‰¹é‡è§£æç›®å½•ä¸‹çš„æ‰€æœ‰æ± å­æ–‡æ¡£
     */
    async parseDirectory(poolsDir, recursive = true) {
        if (!fs.existsSync(poolsDir)) {
            throw new Error(`ç›®å½•ä¸å­˜åœ¨: ${poolsDir}`);
        }
        
        const results = [];
        const mdFiles = this.getMarkdownFiles(poolsDir, recursive);
        
        for (const filePath of mdFiles) {
            const document = await this.parseFile(filePath);
            if (document) {
                results.push(document);
            }
        }
        
        return results;
    }
    
    /**
     * æŒ‰ç±»åˆ«ç­›é€‰æ–‡æ¡£
     */
    filterByCategory(documents, category) {
        return documents.filter(doc => doc.category === category);
    }
    
    /**
     * æŒ‰é£é™©ç­‰çº§ç­›é€‰æ–‡æ¡£
     */
    filterByRisk(documents, risk) {
        return documents.filter(doc => doc.frontmatter.Risk === risk);
    }
    
    /**
     * è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯
     */
    getStatistics(documents) {
        if (!documents || documents.length === 0) {
            return {};
        }
        
        const stats = {
            totalCount: documents.length,
            categories: {},
            riskDistribution: {},
            tokenDistribution: {}
        };
        
        for (const doc of documents) {
            // ç±»åˆ«åˆ†å¸ƒ
            const category = doc.category;
            stats.categories[category] = (stats.categories[category] || 0) + 1;
            
            // é£é™©åˆ†å¸ƒ
            const risk = doc.frontmatter.Risk;
            stats.riskDistribution[risk] = (stats.riskDistribution[risk] || 0) + 1;
            
            // ä»£å¸åˆ†å¸ƒ
            const token = doc.frontmatter.Token;
            stats.tokenDistribution[token] = (stats.tokenDistribution[token] || 0) + 1;
        }
        
        return stats;
    }
    
    /**
     * å¯¼å‡ºä¸º JSON æ ¼å¼
     */
    exportToJSON(documents) {
        return JSON.stringify(documents, null, 2);
    }
}

// ä½¿ç”¨ç¤ºä¾‹
async function main() {
    const parser = new PoolDocumentParser();
    
    // è§£æå•ä¸ªæ–‡ä»¶
    const doc = await parser.parseFile('cex/Binance BNSOL Staking.md');
    if (doc) {
        console.log(`åè®®: ${doc.frontmatter.Protocol}`);
        console.log(`å¸‚åœº: ${JSON.stringify(doc.frontmatter.Market)}`);
        console.log(`æ”¶ç›Šæœºåˆ¶: ${doc.details.Underlying.substring(0, 50)}...`);
    }
    
    // æ‰¹é‡è§£æ
    const allDocs = await parser.parseDirectory('.');
    console.log(`å…±è§£æ ${allDocs.length} ä¸ªæ± å­æ–‡æ¡£`);
    
    // ç»Ÿè®¡ä¿¡æ¯
    const stats = parser.getStatistics(allDocs);
    console.log('é£é™©åˆ†å¸ƒ:', stats.riskDistribution || {});
}

// å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼Œæ‰§è¡Œä¸»å‡½æ•°
if (require.main === module) {
    main().catch(console.error);
}

module.exports = { PoolDocumentParser, main };
```

## ğŸ“ˆ æ€§èƒ½åŸºå‡†

| è§£æå™¨ | å•æ–‡ä»¶è€—æ—¶ | 100æ–‡ä»¶è€—æ—¶ | æ¯ç§’å¤„ç†é‡ |
|--------|------------|-------------|------------|
| Python | ~0.013ms | ~1.3ms | ~76,923 æ–‡ä»¶/ç§’ |
| JavaScript | ~0.010ms | ~1.0ms | **98,572 æ–‡ä»¶/ç§’** |

## ğŸ”§ ä½¿ç”¨æŒ‡å—

### å¿«é€Ÿå¼€å§‹

**Python:**
```bash
# å®‰è£…ä¾èµ–
pip install pyyaml markdown

# è¿è¡Œè§£æå™¨
python pool_parser.py
```

**JavaScript:**
```bash
# å®‰è£…æ ‡å‡†Markdownè§£æåº“
npm install remark remark-html gray-matter

# è¿è¡Œè§£æå™¨
node pool_parser.js
```

### åŸºæœ¬ç”¨æ³•

**Python ç¤ºä¾‹:**
```python
parser = PoolDocumentParser()

# è§£æå•ä¸ªæ–‡ä»¶
doc = parser.parse_file("cex/Binance BNSOL Staking.md")
print(f"åè®®: {doc.frontmatter.Protocol}")
print(f"å¸‚åœº: {doc.frontmatter.Market}")
print(f"æ”¶ç›Šæœºåˆ¶: {doc.details.Underlying}")

# æ‰¹é‡è§£æ
all_docs = parser.parse_directory(".")

# ç­›é€‰å’Œç»Ÿè®¡
cex_docs = parser.filter_by_category(all_docs, 'cex')
stats = parser.get_statistics(all_docs)
```

**JavaScript ç¤ºä¾‹:**
```javascript
const parser = new PoolDocumentParser();

// è§£æå•ä¸ªæ–‡ä»¶
const doc = await parser.parseFile('cex/Binance BNSOL Staking.md');
console.log(`åè®®: ${doc.frontmatter.Protocol}`);
console.log(`å¸‚åœº: ${doc.frontmatter.Market}`);
console.log(`æ”¶ç›Šæœºåˆ¶: ${doc.details.Underlying}`);

// æ‰¹é‡è§£æ
const allDocs = await parser.parseDirectory('.');

// ç­›é€‰å’Œç»Ÿè®¡
const cexDocs = parser.filterByCategory(allDocs, 'cex');
const stats = parser.getStatistics(allDocs);
```

## ğŸ¯ æœ€ä½³å®è·µ

### æ–‡æ¡£ç¼–å†™è§„èŒƒ

1. **å‘½åè§„èŒƒ**: ä½¿ç”¨ `åè®®å ä»£å¸ äº§å“ç±»å‹.md` æ ¼å¼
2. **Frontmatter å®Œæ•´æ€§**: ç¡®ä¿ Protocolã€Typeã€Riskã€Market ç­‰å­—æ®µéƒ½æœ‰å€¼
3. **æ­£æ–‡å†…å®¹è‡ªç”±åº¦**: Underlyingã€Dangerã€Scenarios å¯ä½¿ç”¨ä»»æ„ Markdown æ ¼å¼
4. **æ ¼å¼ä¸€è‡´æ€§**: å­—æ®µæ ‡é¢˜ä½¿ç”¨ `**å­—æ®µå**` æ ¼å¼ï¼Œåè·Ÿç©ºè¡Œ
5. **å†…å®¹å‡†ç¡®æ€§**: å®šæœŸæ›´æ–° APR å’Œé£é™©è¯„ä¼°

### ç¨‹åºè§£æå»ºè®®

1. **ä½¿ç”¨æ ‡å‡†Markdownåº“**: æ¨èä½¿ç”¨ `remark`ã€`markdown` ç­‰æ ‡å‡†åº“è€Œéæ­£åˆ™è¡¨è¾¾å¼
2. **AST-basedè§£æ**: é€šè¿‡è§£æMarkdown ASTç¡®ä¿å®Œæ•´æå–å¤šè¡Œå†…å®¹  
3. **HTMLæ¸²æŸ“**: å°†Markdownè¯­æ³•è½¬æ¢ä¸ºHTMLï¼Œä¿è¯å‰ç«¯æ­£ç¡®æ˜¾ç¤ºæ ¼å¼
4. **å‰ç«¯é›†æˆ**: ä½¿ç”¨`dangerouslySetInnerHTML`ç»“åˆTypographyæ’ä»¶æ­£ç¡®æ˜¾ç¤ºHTMLå†…å®¹
5. **é”™è¯¯å¤„ç†**: æ·»åŠ æ–‡ä»¶ä¸å­˜åœ¨ã€æ ¼å¼é”™è¯¯çš„å¤„ç†ï¼Œæä¾›æ­£åˆ™è¡¨è¾¾å¼é™çº§æ–¹æ¡ˆ
6. **æ€§èƒ½ä¼˜åŒ–**: å¤§æ‰¹é‡å¤„ç†æ—¶è€ƒè™‘å¹¶è¡Œè§£æ
7. **æ•°æ®éªŒè¯**: æ·»åŠ æ•°æ®ç±»å‹å’Œæ ¼å¼éªŒè¯
8. **ç¼“å­˜æœºåˆ¶**: é¢‘ç¹è®¿é—®æ—¶å®ç°ç»“æœç¼“å­˜

### å‰ç«¯é›†æˆæœ€ä½³å®è·µ

**HTMLå†…å®¹æ¸²æŸ“**:
```tsx
// Reactç»„ä»¶ä¸­æ­£ç¡®æ˜¾ç¤ºMarkdownè½¬æ¢çš„HTMLå†…å®¹
<div 
  className="prose prose-sm max-w-none" 
  dangerouslySetInnerHTML={{ __html: htmlContent }}
/>
```

**Typographyé…ç½®**:
```javascript
// tailwind.config.js
module.exports = {
  plugins: [require("@tailwindcss/typography")],
}
```

**å®‰å…¨è€ƒè™‘**:
- HTMLå†…å®¹æ¥æºå¯ä¿¡ï¼ˆæ ‡å‡†Markdownåº“ç”Ÿæˆï¼‰
- ä¸åŒ…å«ç”¨æˆ·è¾“å…¥çš„åŸå§‹HTML
- å¯å®‰å…¨ä½¿ç”¨`dangerouslySetInnerHTML`

## ğŸ“Š å·²è§£ææ•°æ®ç»Ÿè®¡

åŸºäºå½“å‰ pools ç›®å½•çš„å®é™…æ•°æ®ï¼š

- **æ€»æ–‡æ¡£æ•°**: 79 ä¸ª
- **ç±»åˆ«åˆ†å¸ƒ**: CEX (17ä¸ª), Chain (62ä¸ª)
- **é£é™©åˆ†å¸ƒ**: æ— é£é™© (29ä¸ª), ä½é£é™© (16ä¸ª), ä¸­é£é™© (17ä¸ª), é«˜é£é™© (14ä¸ª), æé«˜é£é™© (3ä¸ª)
- **è§£ææˆåŠŸç‡**: 98.7%

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. æ–°å¢æ± å­æ–‡æ¡£è¯·éµå¾ªæ ‡å‡†æ ¼å¼
2. ä¿®æ”¹ç°æœ‰æ–‡æ¡£æ—¶ä¿æŒæ ¼å¼ä¸€è‡´æ€§  
3. æäº¤å‰ä½¿ç”¨è§£æå™¨éªŒè¯æ ¼å¼æ­£ç¡®æ€§
4. é‡å¤§ç»“æ„å˜æ›´è¯·å…ˆè®¨è®º

---

**æ›´æ–°æ—¥æœŸ**: 2025-07-26  
**ç‰ˆæœ¬**: v2.1.0 - æ ‡å‡†Markdownåº“è§£æå™¨  
**ç»´æŠ¤è€…**: @chiangguantik

## ğŸ“ æ›´æ–°æ—¥å¿—

**v2.1.0** (2025-07-26)
- âœ¨ æ–°å¢ï¼šä½¿ç”¨æ ‡å‡†Markdownè§£æåº“ï¼ˆ`remark`, `markdown`ï¼‰æ›¿ä»£æ­£åˆ™è¡¨è¾¾å¼
- âœ¨ æ–°å¢ï¼šAST-basedè§£æç¡®ä¿å®Œæ•´æå–å¤šè¡Œå†…å®¹
- âœ¨ æ–°å¢ï¼šMarkdownåˆ°HTMLçš„è‡ªåŠ¨è½¬æ¢å’Œæ¸²æŸ“
- âœ¨ æ–°å¢ï¼šæ­£åˆ™è¡¨è¾¾å¼é™çº§æ–¹æ¡ˆä½œä¸ºå¤‡ä»½
- âœ¨ æ–°å¢ï¼šå‰ç«¯é›†æˆæœ€ä½³å®è·µå’ŒTypographyé…ç½®æŒ‡å—
- ğŸ”§ ä¼˜åŒ–ï¼šæå‡è§£æå‡†ç¡®æ€§å’Œå¯é æ€§
- ğŸ“š æ–‡æ¡£ï¼šæ›´æ–°è§£æå™¨ä»£ç ç¤ºä¾‹å’Œä½¿ç”¨æŒ‡å—
- ğŸ“š æ–‡æ¡£ï¼šæ·»åŠ HTMLæ¸²æŸ“å’Œå‰ç«¯é›†æˆçš„è¯¦ç»†è¯´æ˜

**v2.0.0** (2025-01-26)
- ğŸ‰ é¦–æ¬¡å‘å¸ƒå®Œæ•´è§£æå™¨ç³»ç»Ÿ